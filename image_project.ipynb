{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imread` from `matplotlib.pyplot`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration items collected into one place\n",
    "cfg = {\n",
    "    # Training files directories\n",
    "    'training_files_root_dir': 'train',\n",
    "    'annotation_dir': 'annotations', # Sub-directory\n",
    "    'images_dir': 'images', # Sub-directory\n",
    "    # Other directories\n",
    "    'checkpoint_dir': \"./checkpoints/\",\n",
    "    'log_dir': \"./logs/\"\n",
    "    # Paths to be used when using Colaboratory\n",
    "    # 'checkpoint_dir': \"/content/gdrive/My Drive/checkpoints/\",\n",
    "    # 'log_dir': \"/content/gdrive/My Drive/logs/\"\n",
    "  }\n",
    "\n",
    "# Constants\n",
    "constants = {\n",
    "    # These are possible labels defined in the dataset\n",
    "    'label_titles': [\"baby\", \"bird\", \"car\", \"clouds\", \"dog\", \"female\", \"flower\",\n",
    "                \"male\", \"night\", \"people\", \"portrait\", \"river\", \"sea\", \"tree\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if data files are present and obtain them only if they are missing\n",
    "if not os.path.isdir(cfg['training_files_root_dir']):\n",
    "    from torchvision.datasets.utils import download_url\n",
    "    import zipfile\n",
    "\n",
    "    # Left here as not used anywhere else\n",
    "    dl_file = 'dl2018-image-proj.zip'\n",
    "    dl_url = 'https://users.aalto.fi/mvsjober/misc/'\n",
    "\n",
    "    zip_path = os.path.join(cfg['training_files_root_dir'], dl_file)\n",
    "    \n",
    "    # Download data file if needed\n",
    "    if not os.path.isfile(zip_path):\n",
    "        download_url(dl_url + dl_file, root=cfg['training_files_root_dir'], filename=dl_file, md5=None)\n",
    "\n",
    "    # Extract data file\n",
    "    with zipfile.ZipFile(zip_path) as zip_f:\n",
    "        zip_f.extractall(cfg['training_files_root_dir'])\n",
    "else:\n",
    "    print('Data is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Data statistics](#data-statistics)\n",
    "\n",
    "1.1. [Peek at some images](#peek-at-some-images)\n",
    "\n",
    "1.2. [Look at label relations](#look-at-label-relations)\n",
    "\n",
    "2. [Data manipulation](#data-manipulation)\n",
    "\n",
    "2.1. [Custom classes and helpers](#custom-classes-and-helpers)\n",
    "\n",
    "2.2. [Dataset creations](#dataset-creations)\n",
    "\n",
    "2.3. [Data loaders](#data-loaders)\n",
    "\n",
    "2.4. [Peek at images](#peek-at-images)\n",
    "\n",
    "3. [Functions for defining and training the model](#functions-for-defining-and-training-the-model)\n",
    "\n",
    "3.1. [One baseline network](#one-baseline-network)\n",
    "\n",
    "3.2. [Network runner](#network-runner)\n",
    "\n",
    "4. [Some of the different network configurations that have been tried](#different-network-configurations-that-have-been-tried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistics <a name=\"data-statistics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports except for os, which is done earlier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "from skimage import io, transform\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from skmultilearn.cluster import LabelCooccurrenceGraphBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_matrix(annotations_dir_name):\n",
    "    # Create labels for use in dataset\n",
    "    labels = np.zeros((20000,14))\n",
    "    \n",
    "    anno_dir = os.fsencode(annotations_dir_name)\n",
    "    \n",
    "    for idx, file in enumerate(sorted(os.listdir(anno_dir))):\n",
    "        filename = os.fsdecode(file)\n",
    "        with open(os.path.join(annotations_dir_name, filename)) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # Image name indexing starts from 1, so convert\n",
    "                # it to matrix index by subtracting one\n",
    "                img_idx = int(line) - 1\n",
    "                labels[img_idx, idx] = 1\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse label data\n",
    "labs = create_label_matrix(os.path.join(cfg['training_files_root_dir'],cfg['annotation_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class statistics\n",
      "\n",
      "Class               Count               % of all labels     % of all samples    \n",
      "baby                95                  0.47%           0.475%\n",
      "river               120                 0.593%             0.6%\n",
      "sea                 173                 0.855%           0.865%\n",
      "car                 319                 1.58%            1.59%\n",
      "bird                360                 1.78%             1.8%\n",
      "dog                 448                 2.22%            2.24%\n",
      "tree                525                 2.6%            2.62%\n",
      "night               598                 2.96%            2.99%\n",
      "flower              761                 3.76%            3.81%\n",
      "clouds              1095                5.41%            5.47%\n",
      "male                2979                14.7%            14.9%\n",
      "portrait            3121                15.4%            15.6%\n",
      "female              3227                16.0%            16.1%\n",
      "people              6403                31.7%            32.0%\n",
      "\n",
      "Total labels        20224\n",
      "\n",
      "Samples with labels    10176 (50.9%)\n",
      "Samples without labels 9824 (49.1%)\n"
     ]
    }
   ],
   "source": [
    "# Label distributions statistics\n",
    "lab_counts = labs.sum(axis=0)\n",
    "total_labs = lab_counts.sum()\n",
    "sorted_indices = np.argsort(lab_counts)\n",
    "labeless = (labs.sum(axis=1)==0).sum()\n",
    "\n",
    "pad = 20\n",
    "print(\"Class statistics\\n\")\n",
    "print(f\"{'Class':<{pad}}{'Count':<{pad}}{'% of all labels':<{pad}}{'% of all samples':<{pad}}\")\n",
    "for i in sorted_indices:\n",
    "    print(f\"{constants['label_titles'][i]:<{pad}}{int(lab_counts[i]):<{pad}}\"+\n",
    "          f\"{lab_counts[i]/total_labs*100:.3}%{lab_counts[i]/len(labs)*100:16.3}%\")\n",
    "print(f\"\\n{'Total labels':<{pad}}{int(total_labs)}\\n\")\n",
    "print(f\"{'Samples with labels':<22} {len(labs)-labeless} ({(len(labs)-labeless)/len(labs)*100:.3}%)\")\n",
    "print(f\"{'Samples without labels':<22} {labeless} ({labeless/len(labs)*100:.3}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample can have this many labels: [0. 1. 2. 3. 4. 5.]\n",
      "Sample counts by label counts from 0 to 5: [9824 4161 2388 3230  388    9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How many labels per sample\n",
    "bins = np.unique(labs.sum(axis=1))\n",
    "print(f\"Sample can have this many labels: {bins}\")\n",
    "counts = np.array([(labs.sum(axis=1)==u).sum() for u in bins])\n",
    "print(f\"Sample counts by label counts from 0 to 5: {counts}\")\n",
    "plt.figure()\n",
    "plt.bar(bins,counts)\n",
    "plt.grid()\n",
    "plt.xlabel('Number of labels')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title(\"Number of samples per label counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at some images <a name=\"peek-at-some-images\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the pictures\n",
    "\n",
    "# Prints pictures with exactly c labels\n",
    "def picture_by_label_count(c, size=5):\n",
    "    inds = np.nonzero((labs.sum(axis=1) == c).astype(int))\n",
    "    inds = np.random.choice(inds[0], size=size, replace=False)\n",
    "    for i in inds:\n",
    "        image = Image.open(os.path.join(cfg['training_files_root_dir'],\n",
    "                                        cfg['images_dir'], f\"im{i+1}.jpg\"))\n",
    "        imlabs = [constants['label_titles'][idx] for idx,l in enumerate(labs[i,:]) if l==1]\n",
    "        plt.imshow(image,label=imlabs)\n",
    "        plt.title(imlabs)\n",
    "        plt.show()\n",
    "    return np.nonzero(inds)\n",
    "\n",
    "# Prints pictures that have label at index c (baby=0,..., tree=13)\n",
    "def picture_by_label_class(c,size=5):\n",
    "    inds = np.nonzero((labs[:,c] == 1).astype(int))\n",
    "    inds = np.random.choice(inds[0], size=size, replace=False)\n",
    "    for i in inds:\n",
    "        image = Image.open(os.path.join(cfg['training_files_root_dir'],\n",
    "                                        cfg['images_dir'], f\"im{i+1}.jpg\"))\n",
    "        imlabs = [constants['label_titles'][idx] for idx,l in enumerate(labs[i,:]) if l==1]\n",
    "        plt.imshow(image,label=imlabs)\n",
    "        plt.title(imlabs)\n",
    "        plt.show()\n",
    "    return np.nonzero(inds)\n",
    "\n",
    "# Pictures with different amount of labels\n",
    "for i in range(6):\n",
    "    print(f\"Pictures with {i} label{'s' if i != 1 else ''}\")\n",
    "    picture_by_label_count(i,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pictures with different classes\n",
    "for i in range(14):\n",
    "    print(f\"Pictures with label {constants['label_titles'][i]}\")\n",
    "    picture_by_label_class(i,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at label relations <a name=\"look-at-label-relations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 labels, 70 edges\n",
      "{(5, 9): 3166.0, (5, 10): 1946.0, (9, 10): 3121.0, (7, 9): 2928.0, (3, 13): 48.0, (8, 9): 78.0, (5, 7): 581.0, (7, 10): 1327.0, (1, 7): 8.0, (1, 9): 15.0, (5, 12): 15.0, (9, 12): 57.0, (3, 5): 31.0, (3, 9): 139.0, (0, 9): 94.0, (5, 8): 11.0, (3, 7): 60.0, (3, 12): 42.0, (7, 12): 30.0, (0, 10): 80.0, (7, 13): 15.0, (9, 13): 45.0, (0, 7): 43.0, (3, 11): 28.0, (1, 13): 16.0, (7, 8): 32.0, (3, 6): 6.0, (8, 13): 23.0, (3, 8): 17.0, (4, 9): 54.0, (0, 5): 23.0, (3, 10): 17.0, (5, 11): 4.0, (9, 11): 14.0, (10, 11): 3.0, (2, 5): 13.0, (2, 7): 20.0, (2, 9): 42.0, (2, 10): 4.0, (8, 10): 10.0, (6, 7): 9.0, (6, 9): 29.0, (6, 10): 18.0, (2, 8): 11.0, (1, 5): 8.0, (11, 13): 13.0, (5, 6): 26.0, (2, 3): 14.0, (2, 12): 1.0, (1, 3): 8.0, (4, 5): 23.0, (6, 13): 25.0, (2, 13): 1.0, (5, 13): 12.0, (4, 7): 27.0, (4, 10): 21.0, (10, 13): 3.0, (3, 4): 2.0, (1, 12): 3.0, (4, 13): 2.0, (4, 12): 2.0, (2, 4): 2.0, (7, 11): 8.0, (11, 12): 1.0, (12, 13): 5.0, (1, 6): 3.0, (1, 10): 5.0, (8, 11): 3.0, (10, 12): 3.0, (4, 6): 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Label relations\n",
    "graph_builder = LabelCooccurrenceGraphBuilder(weighted=True, include_self_edges=False)\n",
    "\n",
    "edge_map = graph_builder.transform(labs)\n",
    "print(\"{} labels, {} edges\".format(len(constants['label_titles']), len(edge_map)))\n",
    "print(edge_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common label co-occurences\n",
      "\n",
      "(female, people): \t3166\n",
      "(people, portrait): \t3121\n",
      "(male, people): \t2928\n",
      "(female, portrait): \t1946\n",
      "(male, portrait): \t1327\n",
      "(female, male): \t581\n",
      "(clouds, people): \t139\n",
      "(baby, people): \t94\n",
      "(baby, portrait): \t80\n",
      "(night, people): \t78\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 most common label co-occurences\\n\")\n",
    "for idx,pair in enumerate(sorted(edge_map,key=edge_map.get, reverse=True)):\n",
    "    if idx==10:\n",
    "        break\n",
    "    print(f\"({constants['label_titles'][pair[0]]}, {constants['label_titles'][pair[1]]}): \\t{int(edge_map[pair])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x221fa49dc18>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "occurence_matrix = np.zeros((14,14))\n",
    "for e in edge_map:\n",
    "    occurence_matrix[e[0],e[1]] = edge_map[e]\n",
    "    occurence_matrix[e[1],e[0]] = edge_map[e]\n",
    "\n",
    "plt.imshow(occurence_matrix,cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation <a name=\"data-manipulation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom classes and helpers <a name=\"custom-classes-and-helpers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for easy access to data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, index_map, transformations, labels, root_dir):\n",
    "        self.index_map = index_map # Contenst in tuple (image number zero based, index to transformations array)\n",
    "        self.labels = labels\n",
    "        self.transformations = transformations # Array of transformations\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Separated image number and transformation index\n",
    "        img_idx, trans_idx = self.index_map[index]\n",
    "        # Image filenames start with 1\n",
    "        filename = f\"im{img_idx + 1}.jpg\"\n",
    "        # Read the image in PIL format for it to work with transformations\n",
    "        image = Image.open(os.path.join(self.root_dir, filename))\n",
    "        imglabels = self.labels[img_idx]\n",
    "        # Apply necessary transformations\n",
    "        image = self.transformations[trans_idx](image)\n",
    "        \n",
    "        # This takes the V value from HSV transformation i.e. does color to grayscale transformation\n",
    "        image = torch.max(image, dim=0)[0].unsqueeze(dim=0)\n",
    "\n",
    "        return (image, imglabels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image and transformation indices into single array, so that all different\n",
    "# transformations are done to image\n",
    "def combine_index_and_transform(indices, trnsfrms):\n",
    "    cidx = []\n",
    "    for tr in range(len(trnsfrms)):\n",
    "        for idx in indices:\n",
    "            cidx.append((idx, tr))\n",
    "    return cidx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for various datasets related items\n",
    "class TVTClass:\n",
    "    size = 0 # Set size\n",
    "    indices = [] # Image indices for this set\n",
    "    transforms = 0 # Array of transformations for these images\n",
    "    index_transform_map = [] # Tuple array of combine_index_and_transform result\n",
    "    img_set = 0 # Dataset class instace\n",
    "    loader = 0 # Dataloader class instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creations <a name=\"dataset-creations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_datasets(ctrain, cvalidation, labels, train_split):\n",
    "    # Create array of indices and split them into two sets - training and validation.\n",
    "    # This is naive as it treats all images as equal.\n",
    "\n",
    "    # Set dataset sizes\n",
    "    image_indices = list(range(len(labels)))\n",
    "    ctrain.size = int(train_split * len(image_indices))\n",
    "    cvalidation.size = len(image_indices) - ctrain.size\n",
    "    \n",
    "    # Randomize what goes into what set\n",
    "    all_indices = np.random.permutation(image_indices)\n",
    "    ctrain.indices, cvalidation.indices = all_indices[:ctrain.size], all_indices[ctrain.size:]\n",
    "\n",
    "    # Here are the transforms that are applied to the images\n",
    "    # All images must be transformed at least to tensor\n",
    "\n",
    "    # Training data related transformations\n",
    "    ctrain.transforms = [\n",
    "        # By uncommenting line below all images are added as is except for normalization and also\n",
    "        # utilizing random transformations.\n",
    "        #transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        # Here grayscale transformation is done on ImageDataset class\n",
    "        transforms.Compose([transforms.RandomHorizontalFlip(p=0.50),\n",
    "                            transforms.RandomRotation(degrees=15, resample=False, expand=False, center=None),\n",
    "                            transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    # To utilize library grayscale conversion comment line above and uncomment line below.\n",
    "    # In addition remember to comment line in ImageDataset class that performs grayscale transformation!\n",
    "    #    transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.RandomHorizontalFlip(p=0.50),\n",
    "    #                        transforms.RandomRotation(degrees=15, resample=False, expand=False, center=None),\n",
    "    #                        transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    ctrain.index_transform_map = combine_index_and_transform(ctrain.indices, ctrain.transforms)\n",
    "\n",
    "    # Validation data related transformations\n",
    "    # Typically nothing else than to tensor\n",
    "    cvalidation.transforms = [\n",
    "        transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    cvalidation.index_transform_map = combine_index_and_transform(cvalidation.indices, cvalidation.transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crafted_datasets(labs, sample_size, train_split, labs_indices, ctrain, cvalidation):\n",
    "    # Create array of indices and split them into two sets - training and validation.\n",
    "    # This performs some balancing actions, thus creates better suited version of datasets.\n",
    "    #\n",
    "    # Inputs:\n",
    "    # labs = return value of create_label_matrix function\n",
    "    # sample_size = number of training samples in each set\n",
    "    # train_split = percentage of samples to be assigned to training set\n",
    "    # labs_indices = list of label indices\n",
    "    # ctrain & cvalidation = storage for training and validation class related data,\n",
    "    #                        see TVTClass for further information\n",
    "\n",
    "    labssets = {}\n",
    "    valset = {}\n",
    "    trainset = {}\n",
    "    # Ensure that some iamges of each label group are in validation set\n",
    "    for i in labs_indices:\n",
    "        # Select images based on one label. Obtain row numbers, which are directly translatable to images,\n",
    "        # for which particular label is present.\n",
    "        labssets[i] = set(np.where(labs[:,i]==1)[0])\n",
    "        # Calculate amount to keep in validation set\n",
    "        valset_size = int((1 - train_split) * len(labssets[i]))\n",
    "        # Select randomly validation set images\n",
    "        valset[i] = set(np.random.choice(list(labssets[i]), valset_size, False))\n",
    "        # Put rest in training set\n",
    "        trainset[i] = labssets[i] - valset[i]\n",
    "\n",
    "    # Handle images without any labels similarly to above\n",
    "    nolab_idx = len(labs_indices)\n",
    "    # Obtain row numbers for those rows that have no labels\n",
    "    labssets[nolab_idx] = set(np.where(~labs.any(axis=1))[0])\n",
    "    # Calculate set size and perform split\n",
    "    valset_size = int((1 - train_split) * len(labssets[nolab_idx]))\n",
    "    valset[nolab_idx] = set(np.random.choice(list(labssets[nolab_idx]), valset_size, False))\n",
    "    trainset[nolab_idx] = labssets[nolab_idx] - valset[nolab_idx]\n",
    "    # Append no label index to send of other indices\n",
    "    labs_indices = np.append(labs_indices, nolab_idx)\n",
    "\n",
    "    # Create training groups of equal size by down- or upsampling as necessary\n",
    "    sampled_labs = {}\n",
    "    for i in labs_indices:\n",
    "        # If training set size is equal or greater than requested sample size\n",
    "        # do not utilize same image twice\n",
    "        if len(trainset[i]) >= sample_size:\n",
    "            repl = False\n",
    "        else:\n",
    "            # Otherwise allow duplicate images\n",
    "            repl = True\n",
    "        sampled_labs[i] = np.random.choice(list(trainset[i]), sample_size, repl)\n",
    "        # Add extra images not used in training to validation set\n",
    "        # Note: this might be a place to tune down e.g. amount of images without labels\n",
    "        if not repl:\n",
    "            valset[i] = valset[i] | (trainset[i] - set(sampled_labs[i]))\n",
    "\n",
    "    # Following can be used if no transformation is wanted to some images as noted in\n",
    "    # code that is in comments below\n",
    "    # normtransforms = [transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "    #                                       transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    # Training set random transformations\n",
    "    rndtransforms = [\n",
    "        transforms.Compose([transforms.RandomHorizontalFlip(p=0.60),\n",
    "                            transforms.RandomRotation(degrees=45, resample=False, expand=False, center=None),\n",
    "                            transforms.RandomResizedCrop(128, scale=(0.70, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "                            transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    # Validation set transformations\n",
    "    valtransforms = [transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "\n",
    "    # Create combined lists of images and transformations for both datasets\n",
    "    train_indices = []\n",
    "    train_indices_combo = []\n",
    "    validation_indices = []\n",
    "    validation_indices_combo = []\n",
    "    for i in labs_indices:\n",
    "        # Can be used to omit transformation e.g. in case training set is adequate size\n",
    "        # if len(trainset[i]) >= sample_size:\n",
    "        #     trnsfrm = normtransforms\n",
    "        # else:\n",
    "        trnsfrm = rndtransforms\n",
    "\n",
    "        # Training set handling\n",
    "        tidx_trnsfrm = combine_index_and_transform(sampled_labs[i], trnsfrm)\n",
    "        train_indices = np.append(train_indices, sampled_labs[i])\n",
    "        train_indices_combo += tidx_trnsfrm\n",
    "        \n",
    "        # Validation set handling\n",
    "        vidx_trnsfrm = combine_index_and_transform(valset[i], valtransforms)\n",
    "        validation_indices += valset[i]\n",
    "        validation_indices_combo += vidx_trnsfrm\n",
    "\n",
    "    # Fill in the data for training and validation related datasets\n",
    "    ctrain.size = len(train_indices)\n",
    "    ctrain.indices = train_indices\n",
    "    ctrain.transforms = rndtransforms\n",
    "    ctrain.index_transform_map = train_indices_combo\n",
    "    cvalidation.size = len(validation_indices)\n",
    "    cvalidation.indices = validation_indices\n",
    "    cvalidation.transforms = valtransforms\n",
    "    cvalidation.index_transform_map = validation_indices_combo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders <a name=\"data-loaders\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create iterators for training and validation datasets\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# Tells how big portion of data stays in the training set\n",
    "TRAIN_SPLIT = 0.75\n",
    "# Indicates size of each group of images in training set\n",
    "TRAIN_SAMPLE_SIZE = 3000\n",
    "\n",
    "labels = create_label_matrix(os.path.join(cfg['training_files_root_dir'],cfg['annotation_dir']))\n",
    "\n",
    "ctrain = TVTClass()\n",
    "cvalidation = TVTClass()\n",
    "\n",
    "# The basic dataset usage\n",
    "plain_datasets(ctrain, cvalidation, labels, TRAIN_SPLIT)\n",
    "\n",
    "# Following helps in debugging whereas all is needed is just list of indices for labels\n",
    "#sorted_indices = np.argsort(labels.sum(axis=0))\n",
    "# The balanced dataset usage\n",
    "#create_crafted_datasets(labels, TRAIN_SAMPLE_SIZE, TRAIN_SPLIT, sorted_indices, ctrain, cvalidation)\n",
    "\n",
    "# Create custom dataset loader classes for training and validation\n",
    "ctrain.img_set = ImageDataset(ctrain.index_transform_map, ctrain.transforms, labels,\n",
    "                             os.path.join(cfg['training_files_root_dir'],cfg['images_dir']))\n",
    "cvalidation.img_set = ImageDataset(cvalidation.index_transform_map, cvalidation.transforms, labels,\n",
    "                                  os.path.join(cfg['training_files_root_dir'],cfg['images_dir']))\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "ctrain.loader = torch.utils.data.DataLoader(dataset=ctrain.img_set, batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "cvalidation.loader = torch.utils.data.DataLoader(dataset=cvalidation.img_set,batch_size=BATCH_SIZE,\n",
    "                                                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek at images <a name=\"peek-at-images\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what one batch has\n",
    "# Useful for checking out the effects of transfomations\n",
    "for idx, (image,label) in enumerate(ctrain.loader):\n",
    "    for i in range(image.shape[0]):\n",
    "        s = []\n",
    "        for j in range(len(label[0])):\n",
    "            if int(label[i,j]) == 1:\n",
    "                s.append(constants['label_titles'][j])\n",
    "        plt.title(s)\n",
    "        plt.imshow(image[i].squeeze(),cmap=\"gray\",label=s)\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what one batch has\n",
    "# Useful for checking out the effects of transfomations\n",
    "for idx, (image,label) in enumerate(cvalidation.loader):\n",
    "    for i in range(image.shape[0]):\n",
    "        s = []\n",
    "        for j in range(len(label[0])):\n",
    "            if int(label[i,j]) == 1:\n",
    "                s.append(constants['label_titles'][j])\n",
    "        plt.title(s)\n",
    "        plt.imshow(image[i].squeeze(),cmap=\"gray\",label=s)\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for defining and training the model <a name=\"functions-for-defining-and-training-the-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multilabeling images\n",
    "\n",
    "Template for the network training procedure, which includes logging in the middle of training epochs,\n",
    "logging each epoch losses and accuracies to a separate file, and saving network and optimizer parameters\n",
    "to a separate file.\n",
    "\"\"\"\n",
    "\n",
    "#Hardware detection\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def batch_accuracy(preds, y):\n",
    "    \"\"\"OBSOLETE!!!\n",
    "    Count accuracy for given batch\n",
    "    Parameters:\n",
    "       preds - values predicted by the model\n",
    "       y - target values\n",
    "    Returns:\n",
    "       Amount of correct answers normalized ie value between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Need to copy to CPU when using GPU\n",
    "    pred = preds.cpu()\n",
    "    target = y.cpu()\n",
    "    \n",
    "    # Naive thresholding, if value at least 0.5 label is predicted as true\n",
    "    pred = pred.round()\n",
    "    \n",
    "    # Exact Match Ratio\n",
    "    # Prediction is correct only if all labels for the sample are correct\n",
    "    return (pred.eq(target).sum(dim=1)==14).float().mean()\n",
    "\n",
    "def threshold(z):\n",
    "    return np.round(z)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, logPerInterval=0.25):\n",
    "    \"\"\"Training method\n",
    "    Parameters:\n",
    "        model - Used neural net\n",
    "        iterator - Iterator for training data\n",
    "        optimizer - Used optimizer\n",
    "        criterion - Used loss function\n",
    "        logPerInterval - Fraction of epoch at which time intermediate logs are printed to output,\n",
    "                         in case one epoch takes too long. Set to zero to make no such logs.\n",
    "    Returns:\n",
    "        Training loss\n",
    "        Training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    preds = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    targets = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    prev = 0\n",
    "    for count, (data, target) in enumerate(iterator):\n",
    "        data = data.to(device)\n",
    "        target = target.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        \n",
    "        loss = criterion(predictions, target)\n",
    "        #acc = batch_accuracy(predictions, target)\n",
    "        cur_end = len(predictions) + prev\n",
    "        preds[prev:cur_end,:] = predictions.cpu().detach().numpy()\n",
    "        targets[prev:cur_end,:] = target\n",
    "        prev = cur_end\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "            \n",
    "        progress = count / len(iterator)\n",
    "        if logPerInterval > 0 and count > 0 and count % (int(len(iterator) * logPerInterval)) == 0:\n",
    "            print(f'    {progress*100:.2f}%: Loss: {epoch_loss/count:.3f}')\n",
    "    \n",
    "    epoch_acc = metrics.f1_score(targets,threshold(preds),average=\"micro\")\n",
    "    return epoch_loss / len(iterator), epoch_acc\n",
    "\n",
    "def evaluate(model, iterator, criterion, return_preds=False):\n",
    "    \"\"\"Evaluation method, used for both validation and test sets\n",
    "    Parameters:\n",
    "        model - Used neural net\n",
    "        iterator - Iterator for training data\n",
    "        criterion - Used loss function\n",
    "    Returns:\n",
    "        Loss\n",
    "        Accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    preds = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    targets = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    prev = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (data,target) in enumerate(iterator,1):\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "            predictions = model(data)\n",
    "            \n",
    "            # Save the predictions and true labels\n",
    "            # for further use\n",
    "            cur_end = len(predictions) + prev\n",
    "            preds[prev:cur_end,:] = predictions\n",
    "            targets[prev:cur_end,:] = target\n",
    "            prev = cur_end\n",
    "            \n",
    "            \n",
    "            loss = criterion(predictions, target)\n",
    "            #acc = batch_accuracy(predictions, target)\n",
    "            epoch_loss += loss.item()\n",
    "            #epoch_acc += acc.item()\n",
    "            \n",
    "    epoch_acc = metrics.f1_score(targets,threshold(preds),average=\"micro\")\n",
    "    if return_preds:\n",
    "        return epoch_loss / len(iterator), epoch_acc, (preds, target)\n",
    "    return epoch_loss / len(iterator), epoch_acc\n",
    "\n",
    "def saveCheckpoint(epoch, bestScore, model, optimizer, filename):\n",
    "    \"\"\"Saves checkpoint to external file\n",
    "    Parameters:\n",
    "        epoch - Current epoch\n",
    "        bestScore - Best validation set f1 score obtained in training\n",
    "        model - Neural net, whose parameters are saved\n",
    "        optimizer - Optimizer state to save\n",
    "        filename - Name of the checkpoint file\n",
    "    \"\"\"\n",
    "    \n",
    "    states = {\n",
    "        'epoch': epoch + 1,\n",
    "        'bestScore': bestScore,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(states, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a model\n",
    "# Needs a model of same structure as parameter\n",
    "def load_model(model, optimizer, filePrefix, timestampToLoad, checkpointPath = \"./checkpoints/\"):  \n",
    "    checkPoint = torch.load(\"{}{}-{}.cpt\".format(checkpointPath, filePrefix, timestampToLoad))\n",
    "    epoch = checkPoint['epoch']\n",
    "    bestScore = checkPoint['bestScore']\n",
    "    model.load_state_dict(checkPoint['model'])\n",
    "    optimizer.load_state_dict(checkPoint['optimizer'])\n",
    "    # Following needs to be reconsidered if not using GPU (comment out maybe?)\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()    \n",
    "    \n",
    "    return model, optimizer, epoch, bestScore\n",
    "\n",
    "# Initialise model if training from scratch and not loading it from file\n",
    "def init_model(dropout=0.5):\n",
    "    model = Net(dropout = dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    return model, optimizer, 0, -1.0\n",
    "\n",
    "def show_plots(train_losses, valid_losses, train_score, valid_score, n_epochs, startEpoch=0):\n",
    "    #Draw plots (only plots current session, there's another script to plot what was written to log)\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), train_losses, label='Training loss')\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), valid_losses, label='Validation loss')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), train_score, label='Training F1 score')\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), valid_score, label='Validation F1 score')\n",
    "    plt.title('F1 score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Used to do both training and evaluation of the model\n",
    "# If save_results is False, doesn't save logs or checkpoints, mainly for Colab\n",
    "def train_and_evaluate(checkpoint, criterion, train_loader, validation_loader, n_epochs, startEpoch=0,\n",
    "                       save_results=True,  saveInterval = 1, checkPointRotation = 3,\n",
    "                       recentFilePrefix=\"net01\", bestFilePrefix=\"best01\"):\n",
    "    \n",
    "    criterion = criterion.to(device)\n",
    "    model = checkpoint[0]\n",
    "    optimizer = checkpoint[1]\n",
    "    startEpoch = checkpoint[2]\n",
    "    best_score = checkpoint[3]\n",
    "\n",
    "    recentSaveCount = 0\n",
    "    bestSaveCount = 0\n",
    "    # Lock in starting datetime as part of filename for this session\n",
    "    timestampToSave = \"{}\".format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for epoch in range(startEpoch, startEpoch + n_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, validation_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_score.append(train_acc)\n",
    "        valid_score.append(valid_acc)\n",
    "        print(f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | \"+\n",
    "              f\"Train Acc: {train_acc:.3f} | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f} |\")\n",
    "        # Log loss and accuracy in case we want to draw graphs\n",
    "        # Each session creates new log file. If you want to append to existing log file,\n",
    "        # just remove the timestamp part from the end of filename.\n",
    "        if save_results:\n",
    "            with open(\"{}{}-{}.txt\".format(cfg['log_dir'], recentFilePrefix, timestampToSave), 'a') as logFile:\n",
    "                logFile.write(\"{} {} {} {} {}\\n\".format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "            #Save recent model\n",
    "            if epoch % saveInterval == 0:\n",
    "                saveCheckpoint(epoch, best_score, model, optimizer, \"{}{}-{}-{}.cpt\".format(\n",
    "                    cfg['checkpoint_dir'], recentFilePrefix, timestampToSave, recentSaveCount % checkPointRotation))\n",
    "                recentSaveCount += 1\n",
    "            #Save best model\n",
    "            if valid_acc > best_score:\n",
    "                saveCheckpoint(epoch, best_score, model, optimizer, \"{}{}-{}-{}.cpt\".format(\n",
    "                    cfg['checkpoint_dir'], bestFilePrefix, timestampToSave, bestSaveCount % checkPointRotation))\n",
    "                best_score = valid_acc\n",
    "                bestSaveCount += 1\n",
    "\n",
    "\n",
    "    #Save one last time at the end of execution\n",
    "    if save_results:\n",
    "        saveCheckpoint(epoch, best_score, model, optimizer, \n",
    "                       \"{}{}-{}-{}.cpt\".format(cfg['checkpoint_dir'], recentFilePrefix, timestampToSave, recentSaveCount % checkPointRotation))    \n",
    "    show_plots(train_losses, valid_losses, train_score, valid_score, n_epochs, startEpoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One baseline network <a name=\"one-baseline-network\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "# Name: 3conv_1fc\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Sort of baseline neural network \"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.fc1_in = 256*9*9\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Three convolutions with batch normalization and max pooling\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        # One fully connected layer with batch normalization and dropout\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network runner <a name=\"network-runner\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# If using this Jupyter notebook in Colaboratory you need to mount Gdrive directory to save logs and checkpoints,\n",
    "# thus uncomment following two lines and remember to provide code it requests!\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "# Number of epochs to run\n",
    "N_EPOCHS = 1\n",
    "\n",
    "params = [0.5]\n",
    "\n",
    "for idx,param in enumerate(params):\n",
    "    # Prefixes for model and log files\n",
    "    filePrefix = f\"tst\"\n",
    "    bestPrefix = f\"tst_best\"\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_and_evaluate(init_model(), criterion, ctrain.loader, cvalidation.loader, N_EPOCHS, recentFilePrefix=filePrefix,\n",
    "                      bestFilePrefix=bestPrefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load some saved model (has to have same architecture as the Net class defined above)\n",
    "and evaluate with it one batch from validation set. Show the pictures and corresponding\n",
    "target labels and labels predicted by the model.\n",
    "\"\"\"\n",
    "filePrefix = \"best01\"\n",
    "timestampToLoad = \"20181216-182441-1\"\n",
    "model = Net(0.5)\n",
    "checkPoint = torch.load(\"{}{}-{}.cpt\".format(cfg['checkpoint_dir'], filePrefix, timestampToLoad))\n",
    "model.load_state_dict(checkPoint['model'])\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    for idx, (data,target) in enumerate(cvalidation.loader,1):\n",
    "        data = data.to(device)\n",
    "        target = target.float().to(device)\n",
    "        predictions = model(data)\n",
    "        for idx, img in enumerate(data):\n",
    "            realLabels = []\n",
    "            for i in range(14):\n",
    "                if target[idx, i] == 1.0:\n",
    "                    realLabels.append(constants['label_titles'][i])\n",
    "            predLabels = []\n",
    "            for i in range(14):\n",
    "                if predictions[idx, i] > 0.5:\n",
    "                    predLabels.append(constants['label_titles'][i])            \n",
    "            plt.imshow(img.squeeze(0))\n",
    "            plt.title('real labels: {} -> pred labels: {}'.format(realLabels, predLabels))\n",
    "            plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of the different network configurations that have been tried <a name=\"different-network-configurations-that-have-been-tried\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 4conv_1fc\n",
    "# new best with 0.708\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        self.conv4_out = 512\n",
    "        \n",
    "        self.fc1_in = 512*8*8\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(self.conv3_out,self.conv4_out,kernel_size=2,stride=1,padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(self.conv4_out)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn5 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Four convolutions with batch normalization and max pooling\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        # One fully connected layer with batch normalization and dropout\n",
    "        x = self.do1(torch.relu(self.bn5(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/4conv_loss.png)\n",
    "![title](figures/4conv_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 2deepconv_1fc\n",
    "# best 0.713 but is extremely slow to train\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        \n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.conv4_out = 128\n",
    "        self.conv5_out = 256\n",
    "        self.conv6_out = 256\n",
    "        \n",
    "        self.conv7_out = 128\n",
    "        \n",
    "        \n",
    "        self.fc1_in = 128*12*12\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Lets try stacked convolutions\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # Dimensionality reduction via 1D convolution\n",
    "        self.conv4 = nn.Conv2d(self.conv3_out,self.conv4_out,kernel_size=1,stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(self.conv4_out)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(self.conv4_out,self.conv5_out,kernel_size=3,stride=1)\n",
    "        self.bn5 = nn.BatchNorm2d(self.conv5_out)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(self.conv5_out,self.conv6_out,kernel_size=3,stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(self.conv6_out)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # Dimensionality reduction via 1D convolution\n",
    "        self.conv7 = nn.Conv2d(self.conv6_out,self.conv7_out,kernel_size=1,stride=1)\n",
    "        self.bn7 = nn.BatchNorm2d(self.conv7_out)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn8 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.relu(self.bn5(self.conv5(x)))\n",
    "        x = torch.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn7(self.conv7(x)))\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn8(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/2deepconv_loss.png)\n",
    "![title](figures/2deepconv_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 3conv_1fc_pad_v1\n",
    "# score 0.694 but didn't converge after 30 epochs\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 128\n",
    "        self.conv2_out = 256\n",
    "        self.conv3_out = 512\n",
    "        \n",
    "        self.fc1_in = 512*11*11\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Three convolutions with batch normalization and max pooling\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        # One fully connected layer with batch normalization and dropout\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/3conv_pad_loss.png)\n",
    "![title](figures/3conv_pad_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: trivial_net\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Trivial CNN just for testing that everything works \"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(16*42*42, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # One convolution with max pooling\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 16*42*42)\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/triv_loss.png)\n",
    "![title](figures/triv_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 3conv_2fc\n",
    "# Used dropout of 0.6\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.fc1_in = 256*9*9\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.fc1_out, self.fc2_out)\n",
    "        self.bn5 = nn.BatchNorm1d(self.fc2_out)\n",
    "        self.do2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc2_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Three convolutions with batch normalization and max pooling\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        # Two fully connected layers with batch normalization and dropout\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        x = self.do2(torch.relu(self.bn5(self.fc2(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".  [See more here on how to load the model back](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) if you want to continue training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You should return your predictions for the test set in a plain text file.  The text file contains one row for each test set image.  Each row contains a binary prediction for each label (separated by a single space), 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "The order of the rows should be according to the numeric order of the image numbers.  In the test set, this means that the first row refers to image `im20001.jpg`, the second to `im20002.jpg`, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the prediction output matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:univ]",
   "language": "python",
   "name": "conda-env-univ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
