{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imread` from `matplotlib.pyplot`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration items collected into one place\n",
    "cfg = {\n",
    "    # Training files directories\n",
    "    'training_files_root_dir': 'train',\n",
    "    'annotation_dir': 'annotations', # Sub-directory\n",
    "    'images_dir': 'images', # Sub-directory\n",
    "    # Other directories\n",
    "    'checkpoint_dir': \"./checkpoints/\",\n",
    "    'log_dir': \"./logs/\"\n",
    "    }\n",
    "\n",
    "# Constants\n",
    "constants = {\n",
    "    # These are possible labels defined in the dataset\n",
    "    'label_titles': [\"baby\", \"bird\", \"car\", \"clouds\", \"dog\", \"female\", \"flower\",\n",
    "                \"male\", \"night\", \"people\", \"portrait\", \"river\", \"sea\", \"tree\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if data files are present and obtain them if missing\n",
    "if not os.path.isdir(cfg['training_files_root_dir']):\n",
    "    from torchvision.datasets.utils import download_url\n",
    "    import zipfile\n",
    "\n",
    "    # Left here as not used anywhere else\n",
    "    dl_file = 'dl2018-image-proj.zip'\n",
    "    dl_url = 'https://users.aalto.fi/mvsjober/misc/'\n",
    "\n",
    "    zip_path = os.path.join(cfg['training_files_root_dir'], dl_file)\n",
    "    # Download data file if needed\n",
    "    if not os.path.isfile(zip_path):\n",
    "        download_url(dl_url + dl_file, root=cfg['training_files_root_dir'], filename=dl_file, md5=None)\n",
    "\n",
    "    # Extract data file\n",
    "    with zipfile.ZipFile(zip_path) as zip_f:\n",
    "        zip_f.extractall(cfg['training_files_root_dir'])\n",
    "else:\n",
    "    print('Data is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets parse and take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports save of os, which is done earlier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "from skimage import io, transform\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_matrix(annotations_dir_name):\n",
    "    # Create labels for use in dataset\n",
    "    labels = np.zeros((20000,14))\n",
    "    \n",
    "    anno_dir = os.fsencode(annotations_dir_name)\n",
    "    \n",
    "    for idx, file in enumerate(sorted(os.listdir(anno_dir))):\n",
    "        filename = os.fsdecode(file)\n",
    "        with open(os.path.join(annotations_dir_name, filename)) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # Image name indexing starts from 1, so convert\n",
    "                # it to matrix index by subtracting one\n",
    "                img_idx = int(line)-1\n",
    "                labels[img_idx, idx] = 1\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse label data\n",
    "labs = create_label_matrix(os.path.join(cfg['training_files_root_dir'],cfg['annotation_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distributions\n",
    "lab_counts = labs.sum(axis=0)\n",
    "total_labs = lab_counts.sum()\n",
    "sorted_indices = np.argsort(lab_counts)\n",
    "labeless = (labs.sum(axis=1)==0).sum()\n",
    "\n",
    "\n",
    "pad = 20\n",
    "print(\"Class statistics\\n\")\n",
    "print(f\"{'Class':<{pad}}{'Count':<{pad}}{'% of all labels':<{pad}}{'% of all samples':<{pad}}\")\n",
    "for i in sorted_indices:\n",
    "    print(f\"{constants['label_titles'][i]:<{pad}}{int(lab_counts[i]):<{pad}}\"+\n",
    "          f\"{lab_counts[i]/total_labs*100:.3}%{lab_counts[i]/len(labs)*100:16.3}%\")\n",
    "print(f\"\\n{'Total labels':<{pad}}{int(total_labs)}\\n\")\n",
    "print(f\"{'Samples with labels':<22} {len(labs)-labeless} ({(len(labs)-labeless)/len(labs)*100:.3}%)\")\n",
    "print(f\"{'Samples without labels':<22} {labeless} ({labeless/len(labs)*100:.3}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many labels per sample\n",
    "bins = np.unique(labs.sum(axis=1))\n",
    "print(f\"Sample can have this many labels: {bins}\")\n",
    "counts = np.array([(labs.sum(axis=1)==u).sum() for u in bins])\n",
    "print(f\"Sample counts by label counts from 0 to 5: {counts}\")\n",
    "plt.figure()\n",
    "plt.bar(bins,counts)\n",
    "plt.grid()\n",
    "plt.title(\"Sample counts by label counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the pictures\n",
    "\n",
    "# Prints pictures with exactly c labels\n",
    "def picture_by_label_count(c,size=5):\n",
    "    inds = np.nonzero((labs.sum(axis=1) == c).astype(int))\n",
    "    inds = np.random.choice(inds[0], size=size, replace=False)\n",
    "    for i in inds:\n",
    "        image = Image.open(os.path.join(cfg['training_files_root_dir'],\n",
    "                                        cfg['images_dir'], f\"im{i+1}.jpg\"))\n",
    "        imlabs = [constants['label_titles'][idx] for idx,l in enumerate(labs[i,:]) if l==1]\n",
    "        plt.imshow(image,label=imlabs)\n",
    "        plt.title(imlabs)\n",
    "        plt.show()\n",
    "    return np.nonzero(inds)\n",
    "\n",
    "# Prints pictures that have label at index c (baby=0,..., tree=13)\n",
    "def picture_by_label_class(c,size=5):\n",
    "    inds = np.nonzero((labs[:,c] == 1).astype(int))\n",
    "    inds = np.random.choice(inds[0], size=size, replace=False)\n",
    "    for i in inds:\n",
    "        image = Image.open(os.path.join(cfg['training_files_root_dir'],\n",
    "                                        cfg['images_dir'], f\"im{i+1}.jpg\"))\n",
    "        imlabs = [constants['label_titles'][idx] for idx,l in enumerate(labs[i,:]) if l==1]\n",
    "        plt.imshow(image,label=imlabs)\n",
    "        plt.title(imlabs)\n",
    "        plt.show()\n",
    "    return np.nonzero(inds)\n",
    "\n",
    "# Pictures with different amount of labels\n",
    "for i in range(6):\n",
    "    print(f\"Pictures with {i} label{'s' if i != 1 else ''}\")\n",
    "    picture_by_label_count(i,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pictures with different classes\n",
    "for i in range(14):\n",
    "    print(f\"Pictures with label {constants['label_titles'][i]}\")\n",
    "    picture_by_label_class(i,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label relations\n",
    "from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\n",
    "\n",
    "\n",
    "graph_builder = LabelCooccurrenceGraphBuilder(weighted=True, include_self_edges=False)\n",
    "\n",
    "edge_map = graph_builder.transform(labs)\n",
    "print(\"{} labels, {} edges\".format(len(constants['label_titles']), len(edge_map)))\n",
    "print(edge_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 most common label co-occurences\\n\")\n",
    "for idx,pair in enumerate(sorted(edge_map,key=edge_map.get, reverse=True)):\n",
    "    if idx==10:\n",
    "        break\n",
    "    print(f\"({constants['label_titles'][pair[0]]}, {constants['label_titles'][pair[1]]}): \\t{int(edge_map[pair])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "occurence_matrix = np.zeros((14,14))\n",
    "for e in edge_map:\n",
    "    occurence_matrix[e[0],e[1]] = edge_map[e]\n",
    "    occurence_matrix[e[1],e[0]] = edge_map[e]\n",
    "\n",
    "plt.imshow(occurence_matrix,cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for easy access to data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, index_map, transformations, labels, root_dir):\n",
    "        self.index_map = index_map # Contenst in tuple (image number zero based, index to transformations array)\n",
    "        self.labels = labels\n",
    "        self.transformations = transformations # Array of transformations\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Separated image number and transformation index\n",
    "        img_idx, trans_idx = self.index_map[index]\n",
    "        # Image filenames start with 1\n",
    "        filename = f\"im{img_idx + 1}.jpg\"\n",
    "        # Read the image in PIL format for it to work with transformations\n",
    "        image = Image.open(os.path.join(self.root_dir, filename))\n",
    "        imglabels = self.labels[img_idx]\n",
    "        # Apply necessary transformations\n",
    "        image = self.transformations[trans_idx](image)\n",
    "        \n",
    "        # This takes the V value from HSV transformation i.e. does color to grayscale transformation\n",
    "        image = torch.max(image, dim=0)[0].unsqueeze(dim=0)\n",
    "\n",
    "        return (image, imglabels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image and transformation indices into combined array, so that all different\n",
    "# transformations are done to image\n",
    "def combine_index_and_transform(indices, trnsfrms):\n",
    "    cidx = []\n",
    "    for tr in range(len(trnsfrms)):\n",
    "        for idx in indices:\n",
    "            cidx.append((idx, tr))\n",
    "    return cidx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for various sets related items\n",
    "class TVTClass:\n",
    "    size = 0 # Set size\n",
    "    indices = [] # Image indices for this set\n",
    "    transforms = 0 # Array of transformations for these images\n",
    "    index_transform_map = [] # Tuple array of combine_index_and_transform result\n",
    "    img_set = 0 # Dataset class instace\n",
    "    loader = 0 # Dataloader class instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create iterators for training and validation datasets\n",
    "\n",
    "# BATCH_SIZE is obvious\n",
    "# TRAIN_SPLIT tells how big portion of data stays in\n",
    "# the training set\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_SPLIT = 0.75\n",
    "\n",
    "labels = create_label_matrix(os.path.join(cfg['training_files_root_dir'],cfg['annotation_dir']))\n",
    "\n",
    "ctrain = TVTClass()\n",
    "cvalidation = TVTClass()\n",
    "\n",
    "# Create array of indices and split them into two sets - training and validation\n",
    "# NOTE: This is simple way to include all images. To change that e.g. for downsampling\n",
    "# or upsampling TVTClass.indices (the last line below where those two indices are set)\n",
    "# is the key, so changing these indices creation will achieve wanted result.\n",
    "image_indices = list(range(len(labels)))\n",
    "ctrain.size = int(TRAIN_SPLIT * len(image_indices))\n",
    "cvalidation.size = len(image_indices) - ctrain.size\n",
    "# Randomize what goes into what set\n",
    "all_indices = np.random.permutation(image_indices)\n",
    "ctrain.indices, cvalidation.indices = all_indices[:ctrain.size], all_indices[ctrain.size:]\n",
    "\n",
    "# Here are the transforms that are applied to the images\n",
    "# Images must be transformed at least to tensor, grayscale conversion is done inside dataset class\n",
    "\n",
    "# Training data related transformations\n",
    "#ctrain.transforms = [transforms.Compose([transforms.ToTensor()]),\n",
    "#           transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "\n",
    "#ctrain.transforms = [transforms.Compose([transforms.Grayscale(num_output_channels=1)\n",
    "#                                         ,transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "#                     transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "#                                      transforms.RandomHorizontalFlip(p=0.50),\n",
    "#                                      transforms.RandomRotation(degrees=15, resample=False, expand=False, center=None),\n",
    "#                                      transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "\n",
    "ctrain.transforms = [transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "                     transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.50),\n",
    "                                      transforms.RandomRotation(degrees=15, resample=False, expand=False, center=None),\n",
    "                                      transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "ctrain.index_transform_map = combine_index_and_transform(ctrain.indices, ctrain.transforms)\n",
    "\n",
    "# Validation data related transformations\n",
    "# Typically nothing else than to tensor\n",
    "cvalidation.transforms = [transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                             ])]\n",
    "#cvalidation.transforms = [transforms.Compose([transforms.Grayscale(num_output_channels=1),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#                                             ])]\n",
    "cvalidation.index_transform_map = combine_index_and_transform(cvalidation.indices, cvalidation.transforms)\n",
    "\n",
    "# Create datasets for training and validation\n",
    "ctrain.img_set = ImageDataset(ctrain.index_transform_map, ctrain.transforms, labels,\n",
    "                             os.path.join(cfg['training_files_root_dir'],cfg['images_dir']))\n",
    "cvalidation.img_set = ImageDataset(cvalidation.index_transform_map, cvalidation.transforms, labels,\n",
    "                                  os.path.join(cfg['training_files_root_dir'],cfg['images_dir']))\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "ctrain.loader = torch.utils.data.DataLoader(dataset=ctrain.img_set, batch_size=BATCH_SIZE,\n",
    "                                           shuffle=False)\n",
    "cvalidation.loader = torch.utils.data.DataLoader(dataset=cvalidation.img_set,batch_size=BATCH_SIZE,\n",
    "                                                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for defining and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multilabeling images\n",
    "\n",
    "Template for the network training procedure, which includes logging in the middle of training epochs,\n",
    "logging each epoch losses and accuracies to a separate file, and saving network and optimizer parameters\n",
    "to a separate file.\n",
    "\"\"\"\n",
    "\n",
    "#Hardware detection\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def batch_accuracy(preds, y):\n",
    "    \"\"\"Count accuracy for given batch\n",
    "    Parameters:\n",
    "       preds - values predicted by the model\n",
    "       y - target values\n",
    "    Returns:\n",
    "       Amount of correct answers normalized ie value between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Need to copy to CPU when using GPU\n",
    "    pred = preds.cpu()\n",
    "    target = y.cpu()\n",
    "    \n",
    "    # Naive thresholding, if value at least 0.5 label is predicted as true\n",
    "    pred = pred.round()\n",
    "    \n",
    "    # Exact Match Ratio\n",
    "    # Prediction is correct only if all labels for the sample are correct\n",
    "    return (pred.eq(target).sum(dim=1)==14).float().mean()\n",
    "\n",
    "def threshold(z):\n",
    "    return np.round(z)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, logPerInterval=0.25):\n",
    "    \"\"\"Training method\n",
    "    Parameters:\n",
    "        model - Used neural net\n",
    "        iterator - Iterator for training data\n",
    "        optimizer - Used optimizer\n",
    "        criterion - Used loss function\n",
    "        logPerInterval - Fraction of epoch at which time intermediate logs are printed to output,\n",
    "                         in case one epoch takes too long. Set to zero to make no such logs.\n",
    "    Returns:\n",
    "        Training loss\n",
    "        Training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for count, (data, target) in enumerate(iterator):\n",
    "        data = data.to(device)\n",
    "        target = target.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        \n",
    "        loss = criterion(predictions, target)\n",
    "        acc = batch_accuracy(predictions, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "            \n",
    "        progress = count / len(iterator)\n",
    "        if logPerInterval > 0 and count > 0 and count % (int(len(iterator) * logPerInterval)) == 0:\n",
    "            print(f'    {progress*100:.2f}%: Loss: {epoch_loss/count:.3f}, Acc: {epoch_acc*100/count:.2f}%')\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, return_preds=False):\n",
    "    \"\"\"Evaluation method, used for both validation and test sets\n",
    "    Parameters:\n",
    "        model - Used neural net\n",
    "        iterator - Iterator for training data\n",
    "        criterion - Used loss function\n",
    "    Returns:\n",
    "        Loss\n",
    "        Accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    preds = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    targets = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    prev = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (data,target) in enumerate(iterator,1):\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "            predictions = model(data)\n",
    "            \n",
    "            # Save the predictions and true labels\n",
    "            # for further use\n",
    "            cur_end = len(predictions) + prev\n",
    "            preds[prev:cur_end,:] = predictions\n",
    "            targets[prev:cur_end,:] = target\n",
    "            prev = cur_end\n",
    "            \n",
    "            \n",
    "            loss = criterion(predictions, target)\n",
    "            acc = batch_accuracy(predictions, target)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    print(\"F1 score for validation set: \", \n",
    "          metrics.f1_score(targets,threshold(preds),average=\"micro\"))\n",
    "    if return_preds:\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator), (preds, target)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def saveCheckpoint(epoch, model, optimizer, filename):\n",
    "    \"\"\"Saves checkpoint to external file\n",
    "    Parameters:\n",
    "        epoch - Current epoch\n",
    "        model - Neural net, whose parameters are saved\n",
    "        optimizer - Optimizer state to save\n",
    "        filename - Name of the checkpoint file\n",
    "    \"\"\"\n",
    "    \n",
    "    states = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(states, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(16*42*42, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 16*42*42)\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.fc1_in = 256*9*9\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a model\n",
    "# Needs a model of same structure as parameter\n",
    "def load_model(model, optimizer, timestampToLoad, checkpointPath = \"./checkpoints/\"):  \n",
    "    checkPoint = torch.load(\"{}{}-{}.cpt\".format(checkpointPath, filePrefix, timestampToLoad))\n",
    "    epoch = checkPoint['epoch']\n",
    "    model.load_state_dict(checkPoint['model'])\n",
    "    optimizer.load_state_dict(checkPoint['optimizer'])\n",
    "    # Following needs to be reconsidered if not using GPU (comment out maybe?)\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()    \n",
    "    \n",
    "    return model, optimizer, epoch\n",
    "\n",
    "# Used to do both training and evaluation of the model\n",
    "# If save_results is False, doesn't save logs or checkpoints, mainly for Colab\n",
    "def train_and_evaluate(model, train_loader, validation_loader, n_epochs, startEpoch=0,\n",
    "                       save_results=True,  saveInterval = 5, checkPointRotation = 3,\n",
    "                       filePrefix=\"net01\", checkpointPath = \"./checkpoints/\",\n",
    "                       logPath = \"./logs/\"):\n",
    "    \n",
    "    saveCount = 0\n",
    "    # Lock in starting datetime as part of filename for this session\n",
    "    timestampToSave = \"{}\".format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "    for epoch in range(startEpoch, startEpoch + n_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, validation_loader, criterion)\n",
    "        print(f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | \"+\n",
    "              f\"Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |\")\n",
    "        # Log loss and accuracy in case we want to draw graphs\n",
    "        # Each session creates new log file. If you want to append to existing log file,\n",
    "        # just remove the timestamp part from the end of filename.\n",
    "        if save_results:\n",
    "            with open(\"{}{}-{}.txt\".format(logPath, filePrefix, timestampToSave), 'a') as logFile:\n",
    "                logFile.write(\"{} {} {} {} {}\\n\".format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "            #Save model\n",
    "            if epoch % saveInterval == 0:\n",
    "                saveCheckpoint(epoch, model, optimizer, \"{}{}-{}-{}.cpt\".format(\n",
    "                    checkpointPath, filePrefix, timestampToSave, saveCount % checkPointRotation))\n",
    "                saveCount += 1\n",
    "\n",
    "    #Save one last time at the end of execution\n",
    "    if save_results:\n",
    "        saveCheckpoint(epoch, model, optimizer, \n",
    "                       \"{}{}-{}-{}.cpt\".format(checkpointPath, filePrefix, timestampToSave, saveCount % checkPointRotation))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create dataset and loaders\n",
    "#\n",
    "# 2. Create or load the network\n",
    "#\n",
    "# 3. Train and evaluate it\n",
    "\n",
    "#Obviously savepaths would be different among us developers,\n",
    "#but I suggest timestamping and naming models to preserve backups\n",
    "#CHKPNT_PTH = \"./checkpoints/\"\n",
    "#LOG_PTH = \"./logs/\"\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "#train_loader = train_loader\n",
    "#validation_loader = validation_loader\n",
    "\n",
    "params = [0.5]\n",
    "\n",
    "for idx,param in enumerate(params):\n",
    "    filePrefix = f\"test_pi{idx}\"\n",
    "    model = Net(dropout = param).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_and_evaluate(model, ctrain.loader, cvalidation.loader, N_EPOCHS, filePrefix=filePrefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obviously savepaths would be different among us developers,\n",
    "#but I suggest timestamping and naming models to preserve backups\n",
    "filePrefix = \"net01\"\n",
    "timestampToLoad = \"20181212-171605-1\"\n",
    "loadModel = False    #If set to true, will load checkpoint from 'checkpointPath/filePrefix-timestampToLoad.cpt'\n",
    "\n",
    "saveInterval = 5    #Save the model after this amount of epochs\n",
    "checkPointRotation = 3    #Amount of checkpoints to rotate over one session\n",
    "\n",
    "#Define hyperparameters here (layer widths, dropout etc)\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 1     #Amount of epochs per session\n",
    "\n",
    "model = Net(DROPOUT)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#I'm assuming all of us are using GPUs, in which case this serialization should work.\n",
    "#If you are switching to CPU, some extra steps are needed, so refer to: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "checkPoint = None\n",
    "if loadModel:\n",
    "    checkPoint = torch.load(\"{}{}-{}.cpt\".format(cfg['checkpoint_dir'], filePrefix, timestampToLoad))\n",
    "    model.load_state_dict(checkPoint['model'])\n",
    "    optimizer.load_state_dict(checkPoint['optimizer'])\n",
    "    #Following needs to be reconsidered if not using GPU (comment out maybe?)\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#train_loader = train_loader\n",
    "#valid_loader = validation_loader\n",
    "#test_loader = ...\n",
    "\n",
    "startEpoch = 0\n",
    "if loadModel:\n",
    "    startEpoch = checkPoint['epoch']\n",
    "saveCount = 0\n",
    "#Lock in starting datetime as part of filename for this session\n",
    "timestampToSave = \"{}\".format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(startEpoch, startEpoch + N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, ctrain.loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, cvalidation.loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    #Log loss and accuracy in case we want to draw graphs\n",
    "    #Each session creates new log file. If you want to append to existing log file, just remove the timestamp part from the end of filename.\n",
    "    with open(\"{}{}-{}.txt\".format(cfg['log_dir'], filePrefix, timestampToSave), 'a') as logFile:\n",
    "        logFile.write(\"{} {} {} {} {}\\n\".format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "    #Save model\n",
    "    if epoch % saveInterval == 0:\n",
    "        saveCheckpoint(epoch, model, optimizer, \"{}{}-{}-{}.cpt\".format(cfg['checkpoint_dir'], filePrefix, timestampToSave, saveCount % checkPointRotation))\n",
    "        saveCount += 1\n",
    "            \n",
    "#Save one last time at the end of execution\n",
    "saveCheckpoint(epoch, model, optimizer, \"{}{}-{}-{}.cpt\".format(cfg['checkpoint_dir'], filePrefix, timestampToSave, saveCount % checkPointRotation))\n",
    "\n",
    "#Draw plots (only plots current session, there's another script to plot what was written to log)\n",
    "plt.plot(np.arange(startEpoch+1,startEpoch + N_EPOCHS+1), train_losses, label='Training loss')\n",
    "plt.plot(np.arange(startEpoch+1,startEpoch + N_EPOCHS+1), valid_losses, label='Validation loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".  [See more here on how to load the model back](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) if you want to continue training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You should return your predictions for the test set in a plain text file.  The text file contains one row for each test set image.  Each row contains a binary prediction for each label (separated by a single space), 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "The order of the rows should be according to the numeric order of the image numbers.  In the test set, this means that the first row refers to image `im20001.jpg`, the second to `im20002.jpg`, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the prediction output matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:univ]",
   "language": "python",
   "name": "conda-env-univ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
