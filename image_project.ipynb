{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imread` from `matplotlib.pyplot`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration items collected into one place\n",
    "cfg = {\n",
    "    # Training files directories\n",
    "    'training_files_root_dir': 'train',\n",
    "    'annotation_dir': 'annotations', # Sub-directory\n",
    "    'images_dir': 'images', # Sub-directory\n",
    "    # Other directories\n",
    "    'checkpoint_dir': \"./checkpoints/\",\n",
    "    'log_dir': \"./logs/\"\n",
    "#COLAB USERS!!! Use these definitions instead\n",
    "#    'checkpoint_dir': \"/content/gdrive/My Drive/checkpoints/\",\n",
    "#    'log_dir': \"/content/gdrive/My Drive/logs/\"\n",
    "  }\n",
    "\n",
    "# Constants\n",
    "constants = {\n",
    "    # These are possible labels defined in the dataset\n",
    "    'label_titles': [\"baby\", \"bird\", \"car\", \"clouds\", \"dog\", \"female\", \"flower\",\n",
    "                \"male\", \"night\", \"people\", \"portrait\", \"river\", \"sea\", \"tree\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if data files are present and obtain them if missing\n",
    "if not os.path.isdir(cfg['training_files_root_dir']):\n",
    "    from torchvision.datasets.utils import download_url\n",
    "    import zipfile\n",
    "\n",
    "    # Left here as not used anywhere else\n",
    "    dl_file = 'dl2018-image-proj.zip'\n",
    "    dl_url = 'https://users.aalto.fi/mvsjober/misc/'\n",
    "\n",
    "    zip_path = os.path.join(cfg['training_files_root_dir'], dl_file)\n",
    "    # Download data file if needed\n",
    "    if not os.path.isfile(zip_path):\n",
    "        download_url(dl_url + dl_file, root=cfg['training_files_root_dir'], filename=dl_file, md5=None)\n",
    "\n",
    "    # Extract data file\n",
    "    with zipfile.ZipFile(zip_path) as zip_f:\n",
    "        zip_f.extractall(cfg['training_files_root_dir'])\n",
    "else:\n",
    "    print('Data is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets parse and take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports save of os, which is done earlier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "from skimage import io, transform\n",
    "import sklearn.metrics as metrics\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_matrix(annotations_dir_name):\n",
    "    # Create labels for use in dataset\n",
    "    labels = np.zeros((20000,14))\n",
    "    \n",
    "    anno_dir = os.fsencode(annotations_dir_name)\n",
    "    \n",
    "    for idx, file in enumerate(sorted(os.listdir(anno_dir))):\n",
    "        filename = os.fsdecode(file)\n",
    "        with open(os.path.join(annotations_dir_name, filename)) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # Image name indexing starts from 1, so convert\n",
    "                # it to matrix index by subtracting one\n",
    "                img_idx = int(line)-1\n",
    "                labels[img_idx, idx] = 1\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse label data\n",
    "labs = create_label_matrix(os.path.join(cfg['training_files_root_dir'],cfg['annotation_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distributions\n",
    "lab_counts = labs.sum(axis=0)\n",
    "total_labs = lab_counts.sum()\n",
    "sorted_indices = np.argsort(lab_counts)\n",
    "labeless = (labs.sum(axis=1)==0).sum()\n",
    "\n",
    "\n",
    "pad = 20\n",
    "print(\"Class statistics\\n\")\n",
    "print(f\"{'Class':<{pad}}{'Count':<{pad}}{'% of all labels':<{pad}}{'% of all samples':<{pad}}\")\n",
    "for i in sorted_indices:\n",
    "    print(f\"{constants['label_titles'][i]:<{pad}}{int(lab_counts[i]):<{pad}}\"+\n",
    "          f\"{lab_counts[i]/total_labs*100:.3}%{lab_counts[i]/len(labs)*100:16.3}%\")\n",
    "print(f\"\\n{'Total labels':<{pad}}{int(total_labs)}\\n\")\n",
    "print(f\"{'Samples with labels':<22} {len(labs)-labeless} ({(len(labs)-labeless)/len(labs)*100:.3}%)\")\n",
    "print(f\"{'Samples without labels':<22} {labeless} ({labeless/len(labs)*100:.3}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many labels per sample\n",
    "bins = np.unique(labs.sum(axis=1))\n",
    "print(f\"Sample can have this many labels: {bins}\")\n",
    "counts = np.array([(labs.sum(axis=1)==u).sum() for u in bins])\n",
    "print(f\"Sample counts by label counts from 0 to 5: {counts}\")\n",
    "plt.figure()\n",
    "plt.bar(bins,counts)\n",
    "plt.grid()\n",
    "plt.title(\"Sample counts by label counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the pictures\n",
    "\n",
    "# Prints pictures with exactly c labels\n",
    "def picture_by_label_count(c,size=5):\n",
    "    inds = np.nonzero((labs.sum(axis=1) == c).astype(int))\n",
    "    inds = np.random.choice(inds[0], size=size, replace=False)\n",
    "    for i in inds:\n",
    "        image = Image.open(os.path.join(cfg['training_files_root_dir'],\n",
    "                                        cfg['images_dir'], f\"im{i+1}.jpg\"))\n",
    "        imlabs = [constants['label_titles'][idx] for idx,l in enumerate(labs[i,:]) if l==1]\n",
    "        plt.imshow(image,label=imlabs)\n",
    "        plt.title(imlabs)\n",
    "        plt.show()\n",
    "    return np.nonzero(inds)\n",
    "\n",
    "# Prints pictures that have label at index c (baby=0,..., tree=13)\n",
    "def picture_by_label_class(c,size=5):\n",
    "    inds = np.nonzero((labs[:,c] == 1).astype(int))\n",
    "    inds = np.random.choice(inds[0], size=size, replace=False)\n",
    "    for i in inds:\n",
    "        image = Image.open(os.path.join(cfg['training_files_root_dir'],\n",
    "                                        cfg['images_dir'], f\"im{i+1}.jpg\"))\n",
    "        imlabs = [constants['label_titles'][idx] for idx,l in enumerate(labs[i,:]) if l==1]\n",
    "        plt.imshow(image,label=imlabs)\n",
    "        plt.title(imlabs)\n",
    "        plt.show()\n",
    "    return np.nonzero(inds)\n",
    "\n",
    "# Pictures with different amount of labels\n",
    "for i in range(6):\n",
    "    print(f\"Pictures with {i} label{'s' if i != 1 else ''}\")\n",
    "    picture_by_label_count(i,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pictures with different classes\n",
    "for i in range(14):\n",
    "    print(f\"Pictures with label {constants['label_titles'][i]}\")\n",
    "    picture_by_label_class(i,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label relations\n",
    "from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\n",
    "\n",
    "\n",
    "graph_builder = LabelCooccurrenceGraphBuilder(weighted=True, include_self_edges=False)\n",
    "\n",
    "edge_map = graph_builder.transform(labs)\n",
    "print(\"{} labels, {} edges\".format(len(constants['label_titles']), len(edge_map)))\n",
    "print(edge_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 most common label co-occurences\\n\")\n",
    "for idx,pair in enumerate(sorted(edge_map,key=edge_map.get, reverse=True)):\n",
    "    if idx==10:\n",
    "        break\n",
    "    print(f\"({constants['label_titles'][pair[0]]}, {constants['label_titles'][pair[1]]}): \\t{int(edge_map[pair])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "occurence_matrix = np.zeros((14,14))\n",
    "for e in edge_map:\n",
    "    occurence_matrix[e[0],e[1]] = edge_map[e]\n",
    "    occurence_matrix[e[1],e[0]] = edge_map[e]\n",
    "\n",
    "plt.imshow(occurence_matrix,cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for easy access to data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, index_map, transformations, labels, root_dir):\n",
    "        self.index_map = index_map # Contenst in tuple (image number zero based, index to transformations array)\n",
    "        self.labels = labels\n",
    "        self.transformations = transformations # Array of transformations\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Separated image number and transformation index\n",
    "        img_idx, trans_idx = self.index_map[index]\n",
    "        # Image filenames start with 1\n",
    "        filename = f\"im{img_idx + 1}.jpg\"\n",
    "        # Read the image in PIL format for it to work with transformations\n",
    "        image = Image.open(os.path.join(self.root_dir, filename))\n",
    "        imglabels = self.labels[img_idx]\n",
    "        # Apply necessary transformations\n",
    "        image = self.transformations[trans_idx](image)\n",
    "        \n",
    "        # This takes the V value from HSV transformation i.e. does color to grayscale transformation\n",
    "        image = torch.max(image, dim=0)[0].unsqueeze(dim=0)\n",
    "\n",
    "        return (image, imglabels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image and transformation indices into combined array, so that all different\n",
    "# transformations are done to image\n",
    "def combine_index_and_transform(indices, trnsfrms):\n",
    "    cidx = []\n",
    "    for tr in range(len(trnsfrms)):\n",
    "        for idx in indices:\n",
    "            cidx.append((idx, tr))\n",
    "    return cidx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for various sets related items\n",
    "class TVTClass:\n",
    "    size = 0 # Set size\n",
    "    indices = [] # Image indices for this set\n",
    "    transforms = 0 # Array of transformations for these images\n",
    "    index_transform_map = [] # Tuple array of combine_index_and_transform result\n",
    "    img_set = 0 # Dataset class instace\n",
    "    loader = 0 # Dataloader class instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_datasets(ctrain, cvalidation, labels, train_split):\n",
    "    # Create array of indices and split them into two sets - training and validation\n",
    "    # NOTE: This is simple way to include all images. To change that e.g. for downsampling\n",
    "    # or upsampling TVTClass.indices (the last line below where those two indices are set)\n",
    "    # is the key, so changing these indices creation will achieve wanted result.\n",
    "    image_indices = list(range(len(labels)))\n",
    "    ctrain.size = int(train_split * len(image_indices))\n",
    "    cvalidation.size = len(image_indices) - ctrain.size\n",
    "    # Randomize what goes into what set\n",
    "    all_indices = np.random.permutation(image_indices)\n",
    "    ctrain.indices, cvalidation.indices = all_indices[:ctrain.size], all_indices[ctrain.size:]\n",
    "\n",
    "    # Here are the transforms that are applied to the images\n",
    "    # Images must be transformed at least to tensor, grayscale conversion is done inside dataset class\n",
    "\n",
    "    # Training data related transformations\n",
    "    ctrain.transforms = [\n",
    "        transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
    "        transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.RandomHorizontalFlip(p=0.50),\n",
    "                            transforms.RandomRotation(degrees=15, resample=False, expand=False, center=None),\n",
    "                            transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    ctrain.index_transform_map = combine_index_and_transform(ctrain.indices, ctrain.transforms)\n",
    "\n",
    "    # Validation data related transformations\n",
    "    # Typically nothing else than to tensor\n",
    "    cvalidation.transforms = [\n",
    "        transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    cvalidation.index_transform_map = combine_index_and_transform(cvalidation.indices, cvalidation.transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labs = return value of create_label_matrix function\n",
    "# sample_size = number of training samples in each set\n",
    "# train_split = percentage of samples to be assigned to training set\n",
    "# labs_indices = list of label indices\n",
    "def create_crafted_datasets(labs, sample_size, train_split, labs_indices, ctrain, cvalidation):\n",
    "    labssets = {}\n",
    "    valset = {}\n",
    "    trainset = {}\n",
    "    # Split available images, so that some of each label group are in validation set\n",
    "    for i in labs_indices:\n",
    "        # Select images based on one label. Obtain row numbers, which are directly translatable to images,\n",
    "        # for which particular label is present.\n",
    "        labssets[i] = set(np.where(labs[:,i]==1)[0])\n",
    "        # Calculate amount to keep in validation set\n",
    "        valset_size = int((1 - train_split) * len(labssets[i]))\n",
    "        # Select randomly validation set images\n",
    "        valset[i] = set(np.random.choice(list(labssets[i]), valset_size, False))\n",
    "        # Put rest in training set\n",
    "        trainset[i] = labssets[i] - valset[i]\n",
    "\n",
    "    # Handle images without any labels similarly to above and add them to same sets as last\n",
    "    nolab_idx = len(labs_indices)\n",
    "    # Obtain row numbers for those rows that have no ones in any column\n",
    "    labssets[nolab_idx] = set(np.where(~labs.any(axis=1))[0])\n",
    "    valset_size = int((1 - train_split) * len(labssets[nolab_idx]))\n",
    "    valset[nolab_idx] = set(np.random.choice(list(labssets[nolab_idx]), valset_size, False))\n",
    "    trainset[nolab_idx] = labssets[nolab_idx] - valset[nolab_idx]\n",
    "    # Append no label index to send of other indices\n",
    "    labs_indices = np.append(labs_indices, nolab_idx)\n",
    "\n",
    "    # Create training groups of equal size by down- or upsampling as necessary\n",
    "    sampled_labs = {}\n",
    "    for i in labs_indices:\n",
    "        # If training set size is equal or greater than requested sample size\n",
    "        # do not utilize same image twice\n",
    "        if len(trainset[i]) >= sample_size:\n",
    "            repl = False\n",
    "        else:\n",
    "            repl = True\n",
    "        sampled_labs[i] = np.random.choice(list(trainset[i]), sample_size, repl)\n",
    "        # Add extra images not used in training to validation set\n",
    "        # Note: this might be a place to tune down e.g. amount of images without labels\n",
    "        if not repl:\n",
    "            valset[i] = valset[i] | (trainset[i] - set(sampled_labs[i]))\n",
    "\n",
    "    # Following can be used if no transformation is wanted to some images as noted in\n",
    "    # code that is in comments below\n",
    "    # normtransforms = [transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "    #                                       transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    # Training set random transformations\n",
    "    rndtransforms = [\n",
    "        transforms.Compose([transforms.RandomHorizontalFlip(p=0.60),\n",
    "                            transforms.RandomRotation(degrees=45, resample=False, expand=False, center=None),\n",
    "                            transforms.RandomResizedCrop(128, scale=(0.70, 1.0), ratio=(0.75, 1.3333333333333333)),\n",
    "                            transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "    # Validation set transforms\n",
    "    valtransforms = [transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])]\n",
    "\n",
    "    # Create combined lists of images and transformations for both datasets\n",
    "    train_indices = []\n",
    "    train_indices_combo = []\n",
    "    validation_indices = []\n",
    "    validation_indices_combo = []\n",
    "    for i in labs_indices:\n",
    "        # Can be used to omit transformation e.g. in case training set is adequate size\n",
    "        # if len(trainset[i]) >= sample_size:\n",
    "        #     trnsfrm = normtransforms\n",
    "        # else:\n",
    "        trnsfrm = rndtransforms\n",
    "        # Training set handling\n",
    "        tidx_trnsfrm = combine_index_and_transform(sampled_labs[i], trnsfrm)\n",
    "        train_indices = np.append(train_indices, sampled_labs[i])\n",
    "        train_indices_combo += tidx_trnsfrm\n",
    "        # Validation set handling\n",
    "        vidx_trnsfrm = combine_index_and_transform(valset[i], valtransforms)\n",
    "        validation_indices += valset[i]\n",
    "        validation_indices_combo += vidx_trnsfrm\n",
    "\n",
    "    # Fill in the settings for training and validation\n",
    "    ctrain.size = len(train_indices)\n",
    "    ctrain.indices = train_indices\n",
    "    ctrain.transforms = rndtransforms\n",
    "    ctrain.index_transform_map = train_indices_combo\n",
    "    cvalidation.size = len(validation_indices)\n",
    "    cvalidation.indices = validation_indices\n",
    "    cvalidation.transforms = valtransforms\n",
    "    cvalidation.index_transform_map = validation_indices_combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create iterators for training and validation datasets\n",
    "\n",
    "# BATCH_SIZE is obvious\n",
    "# TRAIN_SPLIT tells how big portion of data stays in the training set\n",
    "# TRAIN_SAMPLE_SIZE indicates size of each group of labels in training set\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SPLIT = 0.75\n",
    "TRAIN_SAMPLE_SIZE = 3000\n",
    "\n",
    "labels = create_label_matrix(os.path.join(cfg['training_files_root_dir'],cfg['annotation_dir']))\n",
    "\n",
    "ctrain = TVTClass()\n",
    "cvalidation = TVTClass()\n",
    "\n",
    "# The basic dataset usage\n",
    "#plain_datasets(ctrain, cvalidation, labels, TRAIN_SPLIT)\n",
    "\n",
    "# Following helps in debugging whereas all is needed is just list of indices for labels\n",
    "sorted_indices = np.argsort(labels.sum(axis=0))\n",
    "create_crafted_datasets(labels, TRAIN_SAMPLE_SIZE, TRAIN_SPLIT, sorted_indices, ctrain, cvalidation)\n",
    "\n",
    "# Create datasets for training and validation\n",
    "ctrain.img_set = ImageDataset(ctrain.index_transform_map, ctrain.transforms, labels,\n",
    "                             os.path.join(cfg['training_files_root_dir'],cfg['images_dir']))\n",
    "cvalidation.img_set = ImageDataset(cvalidation.index_transform_map, cvalidation.transforms, labels,\n",
    "                                  os.path.join(cfg['training_files_root_dir'],cfg['images_dir']))\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "ctrain.loader = torch.utils.data.DataLoader(dataset=ctrain.img_set, batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "cvalidation.loader = torch.utils.data.DataLoader(dataset=cvalidation.img_set,batch_size=BATCH_SIZE,\n",
    "                                                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what one batch has\n",
    "# Useful for checking out the effects of transfomations\n",
    "for idx, (image,label) in enumerate(ctrain.loader):\n",
    "    for i in range(image.shape[0]):\n",
    "        s = []\n",
    "        for j in range(len(label[0])):\n",
    "            if int(label[i,j]) == 1:\n",
    "                s.append(constants['label_titles'][j])\n",
    "        plt.title(s)\n",
    "        plt.imshow(image[i].squeeze(),cmap=\"gray\",label=s)\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what one batch has\n",
    "# Useful for checking out the effects of transfomations\n",
    "for idx, (image,label) in enumerate(cvalidation.loader):\n",
    "    for i in range(image.shape[0]):\n",
    "        s = []\n",
    "        for j in range(len(label[0])):\n",
    "            if int(label[i,j]) == 1:\n",
    "                s.append(constants['label_titles'][j])\n",
    "        plt.title(s)\n",
    "        plt.imshow(image[i].squeeze(),cmap=\"gray\",label=s)\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for defining and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multilabeling images\n",
    "\n",
    "Template for the network training procedure, which includes logging in the middle of training epochs,\n",
    "logging each epoch losses and accuracies to a separate file, and saving network and optimizer parameters\n",
    "to a separate file.\n",
    "\"\"\"\n",
    "\n",
    "#Hardware detection\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def batch_accuracy(preds, y):\n",
    "    \"\"\"OBSOLETE!!!\n",
    "    Count accuracy for given batch\n",
    "    Parameters:\n",
    "       preds - values predicted by the model\n",
    "       y - target values\n",
    "    Returns:\n",
    "       Amount of correct answers normalized ie value between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Need to copy to CPU when using GPU\n",
    "    pred = preds.cpu()\n",
    "    target = y.cpu()\n",
    "    \n",
    "    # Naive thresholding, if value at least 0.5 label is predicted as true\n",
    "    pred = pred.round()\n",
    "    \n",
    "    # Exact Match Ratio\n",
    "    # Prediction is correct only if all labels for the sample are correct\n",
    "    return (pred.eq(target).sum(dim=1)==14).float().mean()\n",
    "\n",
    "def threshold(z):\n",
    "    return np.round(z)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, logPerInterval=0.25):\n",
    "    \"\"\"Training method\n",
    "    Parameters:\n",
    "        model - Used neural net\n",
    "        iterator - Iterator for training data\n",
    "        optimizer - Used optimizer\n",
    "        criterion - Used loss function\n",
    "        logPerInterval - Fraction of epoch at which time intermediate logs are printed to output,\n",
    "                         in case one epoch takes too long. Set to zero to make no such logs.\n",
    "    Returns:\n",
    "        Training loss\n",
    "        Training accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    preds = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    targets = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    prev = 0\n",
    "    for count, (data, target) in enumerate(iterator):\n",
    "        data = data.to(device)\n",
    "        target = target.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        \n",
    "        loss = criterion(predictions, target)\n",
    "        #acc = batch_accuracy(predictions, target)\n",
    "        cur_end = len(predictions) + prev\n",
    "        preds[prev:cur_end,:] = predictions.cpu().detach().numpy()\n",
    "        targets[prev:cur_end,:] = target\n",
    "        prev = cur_end\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "            \n",
    "        progress = count / len(iterator)\n",
    "        if logPerInterval > 0 and count > 0 and count % (int(len(iterator) * logPerInterval)) == 0:\n",
    "            print(f'    {progress*100:.2f}%: Loss: {epoch_loss/count:.3f}')\n",
    "    \n",
    "    epoch_acc = metrics.f1_score(targets,threshold(preds),average=\"micro\")\n",
    "    return epoch_loss / len(iterator), epoch_acc\n",
    "\n",
    "def evaluate(model, iterator, criterion, return_preds=False):\n",
    "    \"\"\"Evaluation method, used for both validation and test sets\n",
    "    Parameters:\n",
    "        model - Used neural net\n",
    "        iterator - Iterator for training data\n",
    "        criterion - Used loss function\n",
    "    Returns:\n",
    "        Loss\n",
    "        Accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    preds = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    targets = np.zeros((len(iterator)*iterator.batch_size,14))\n",
    "    prev = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (data,target) in enumerate(iterator,1):\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "            predictions = model(data)\n",
    "            \n",
    "            # Save the predictions and true labels\n",
    "            # for further use\n",
    "            cur_end = len(predictions) + prev\n",
    "            preds[prev:cur_end,:] = predictions\n",
    "            targets[prev:cur_end,:] = target\n",
    "            prev = cur_end\n",
    "            \n",
    "            \n",
    "            loss = criterion(predictions, target)\n",
    "            #acc = batch_accuracy(predictions, target)\n",
    "            epoch_loss += loss.item()\n",
    "            #epoch_acc += acc.item()\n",
    "            \n",
    "    epoch_acc = metrics.f1_score(targets,threshold(preds),average=\"micro\")\n",
    "    if return_preds:\n",
    "        return epoch_loss / len(iterator), epoch_acc, (preds, target)\n",
    "    return epoch_loss / len(iterator), epoch_acc\n",
    "\n",
    "def saveCheckpoint(epoch, bestScore, model, optimizer, filename):\n",
    "    \"\"\"Saves checkpoint to external file\n",
    "    Parameters:\n",
    "        epoch - Current epoch\n",
    "        bestScore - Best validation set f1 score obtained in training\n",
    "        model - Neural net, whose parameters are saved\n",
    "        optimizer - Optimizer state to save\n",
    "        filename - Name of the checkpoint file\n",
    "    \"\"\"\n",
    "    \n",
    "    states = {\n",
    "        'epoch': epoch + 1,\n",
    "        'bestScore': bestScore,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(states, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "# Name: 3conv_1fc\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.fc1_in = 256*9*9\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a model\n",
    "# Needs a model of same structure as parameter\n",
    "def load_model(model, optimizer, filePrefix, timestampToLoad, checkpointPath = \"./checkpoints/\"):  \n",
    "    checkPoint = torch.load(\"{}{}-{}.cpt\".format(checkpointPath, filePrefix, timestampToLoad))\n",
    "    epoch = checkPoint['epoch']\n",
    "    bestScore = checkPoint['bestScore']\n",
    "    model.load_state_dict(checkPoint['model'])\n",
    "    optimizer.load_state_dict(checkPoint['optimizer'])\n",
    "    # Following needs to be reconsidered if not using GPU (comment out maybe?)\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()    \n",
    "    \n",
    "    return model, optimizer, epoch, bestScore\n",
    "\n",
    "# Initialise model if training from scratch and not loading it from file\n",
    "def init_model(dropout=0.5):\n",
    "    model = Net(dropout = dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    return model, optimizer, 0, -1.0\n",
    "\n",
    "def show_plots(train_losses, valid_losses, train_score, valid_score, n_epochs, startEpoch=0):\n",
    "    #Draw plots (only plots current session, there's another script to plot what was written to log)\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), train_losses, label='Training loss')\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), valid_losses, label='Validation loss')\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), train_score, label='Training F1 score')\n",
    "    plt.plot(np.arange(startEpoch+1,startEpoch + n_epochs+1), valid_score, label='Validation F1 score')\n",
    "    plt.title('F1 score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Used to do both training and evaluation of the model\n",
    "# If save_results is False, doesn't save logs or checkpoints, mainly for Colab\n",
    "def train_and_evaluate(checkpoint, criterion, train_loader, validation_loader, n_epochs, startEpoch=0,\n",
    "                       save_results=True,  saveInterval = 1, checkPointRotation = 3,\n",
    "                       recentFilePrefix=\"net01\", bestFilePrefix=\"best01\"):\n",
    "    \n",
    "    criterion = criterion.to(device)\n",
    "    model = checkpoint[0]\n",
    "    optimizer = checkpoint[1]\n",
    "    startEpoch = checkpoint[2]\n",
    "    best_score = checkpoint[3]\n",
    "\n",
    "    recentSaveCount = 0\n",
    "    bestSaveCount = 0\n",
    "    # Lock in starting datetime as part of filename for this session\n",
    "    timestampToSave = \"{}\".format(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_score = []\n",
    "    valid_score = []\n",
    "\n",
    "    for epoch in range(startEpoch, startEpoch + n_epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, validation_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        train_score.append(train_acc)\n",
    "        valid_score.append(valid_acc)\n",
    "        print(f\"| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | \"+\n",
    "              f\"Train Acc: {train_acc:.3f} | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc:.3f} |\")\n",
    "        # Log loss and accuracy in case we want to draw graphs\n",
    "        # Each session creates new log file. If you want to append to existing log file,\n",
    "        # just remove the timestamp part from the end of filename.\n",
    "        if save_results:\n",
    "            with open(\"{}{}-{}.txt\".format(cfg['log_dir'], recentFilePrefix, timestampToSave), 'a') as logFile:\n",
    "                logFile.write(\"{} {} {} {} {}\\n\".format(epoch+1, train_loss, train_acc, valid_loss, valid_acc))\n",
    "\n",
    "            #Save recent model\n",
    "            if epoch % saveInterval == 0:\n",
    "                saveCheckpoint(epoch, best_score, model, optimizer, \"{}{}-{}-{}.cpt\".format(\n",
    "                    cfg['checkpoint_dir'], recentFilePrefix, timestampToSave, recentSaveCount % checkPointRotation))\n",
    "                recentSaveCount += 1\n",
    "            #Save best model\n",
    "            if valid_acc > best_score:\n",
    "                saveCheckpoint(epoch, best_score, model, optimizer, \"{}{}-{}-{}.cpt\".format(\n",
    "                    cfg['checkpoint_dir'], bestFilePrefix, timestampToSave, bestSaveCount % checkPointRotation))\n",
    "                best_score = valid_acc\n",
    "                bestSaveCount += 1\n",
    "\n",
    "\n",
    "    #Save one last time at the end of execution\n",
    "    if save_results:\n",
    "        saveCheckpoint(epoch, best_score, model, optimizer, \n",
    "                       \"{}{}-{}-{}.cpt\".format(cfg['checkpoint_dir'], recentFilePrefix, timestampToSave, recentSaveCount % checkPointRotation))    \n",
    "    show_plots(train_losses, valid_losses, train_score, valid_score, n_epochs, startEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1. Create dataset and loaders\n",
    "#\n",
    "# 2. Create or load the network\n",
    "#\n",
    "# 3. Train and evaluate it\n",
    "\n",
    "#Obviously savepaths would be different among us developers,\n",
    "#but I suggest timestamping and naming models to preserve backups\n",
    "#CHKPNT_PTH = \"./checkpoints/\"\n",
    "#LOG_PTH = \"./logs/\"\n",
    "\n",
    "# COLAB USERS!! Uncomment this to mount gdrive directory to save logs and checkpoints\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "N_EPOCHS = 35\n",
    "\n",
    "#train_loader = train_loader\n",
    "#validation_loader = validation_loader\n",
    "\n",
    "params = [0.5]\n",
    "\n",
    "for idx,param in enumerate(params):\n",
    "    filePrefix = f\"2deepconv_1fc\"\n",
    "    bestPrefix = f\"2deepconv_1fc_best\"\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    train_and_evaluate(init_model(), criterion, ctrain.loader, cvalidation.loader, N_EPOCHS, recentFilePrefix=filePrefix,\n",
    "                      bestFilePrefix=bestPrefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load some saved model (has to have same architecture as the Net class defined above)\n",
    "and evaluate with it one batch from validation set. Show the pictures and corresponding\n",
    "target labels and labels predicted by the model.\n",
    "\"\"\"\n",
    "filePrefix = \"best01\"\n",
    "timestampToLoad = \"20181216-182441-1\"\n",
    "model = Net(0.5)\n",
    "checkPoint = torch.load(\"{}{}-{}.cpt\".format(cfg['checkpoint_dir'], filePrefix, timestampToLoad))\n",
    "model.load_state_dict(checkPoint['model'])\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    for idx, (data,target) in enumerate(cvalidation.loader,1):\n",
    "        data = data.to(device)\n",
    "        target = target.float().to(device)\n",
    "        predictions = model(data)\n",
    "        for idx, img in enumerate(data):\n",
    "            realLabels = []\n",
    "            for i in range(14):\n",
    "                if target[idx, i] == 1.0:\n",
    "                    realLabels.append(constants['label_titles'][i])\n",
    "            predLabels = []\n",
    "            for i in range(14):\n",
    "                if predictions[idx, i] > 0.5:\n",
    "                    predLabels.append(constants['label_titles'][i])            \n",
    "            plt.imshow(img.squeeze(0))\n",
    "            plt.title('real labels: {} -> pred labels: {}'.format(realLabels, predLabels))\n",
    "            plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different network configs that were tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 4conv_1fc\n",
    "# new best with 0.708\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        self.conv4_out = 512\n",
    "        \n",
    "        self.fc1_in = 512*8*8\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(self.conv3_out,self.conv4_out,kernel_size=2,stride=1,padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(self.conv4_out)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn5 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn5(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Name: 2deepconv_1fc\n",
    "# best 0.713 but is extremely slow to train\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        \n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.conv4_out = 128\n",
    "        self.conv5_out = 256\n",
    "        self.conv6_out = 256\n",
    "        \n",
    "        self.conv7_out = 128\n",
    "        \n",
    "        \n",
    "        self.fc1_in = 128*12*12\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Lets try stacked convolutions\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # Dimensionality reduction via 1D convolution\n",
    "        self.conv4 = nn.Conv2d(self.conv3_out,self.conv4_out,kernel_size=1,stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(self.conv4_out)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(self.conv4_out,self.conv5_out,kernel_size=3,stride=1)\n",
    "        self.bn5 = nn.BatchNorm2d(self.conv5_out)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(self.conv5_out,self.conv6_out,kernel_size=3,stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(self.conv6_out)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # Dimensionality reduction via 1D convolution\n",
    "        self.conv7 = nn.Conv2d(self.conv6_out,self.conv7_out,kernel_size=1,stride=1)\n",
    "        self.bn7 = nn.BatchNorm2d(self.conv7_out)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn8 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.relu(self.bn5(self.conv5(x)))\n",
    "        x = torch.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn7(self.conv7(x)))\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn8(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 3conv_1fc_pad_v1\n",
    "# score 0.694 but didn't converge after 30 epochs\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 128\n",
    "        self.conv2_out = 256\n",
    "        self.conv3_out = 512\n",
    "        \n",
    "        self.fc1_in = 512*11*11\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc1_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trivial_net\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=1,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(16*42*42, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 16*42*42)\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: 3conv_2fc\n",
    "# Used dropout of 0.6\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Neural network used for this project \"\"\"\n",
    "    def __init__(self, dropout):  #TODO: Have more hyperparameters as arguments\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_ch = 1\n",
    "        self.conv1_out = 64\n",
    "        self.conv2_out = 128\n",
    "        self.conv3_out = 256\n",
    "        \n",
    "        self.fc1_in = 256*9*9\n",
    "        self.fc1_out = 1000\n",
    "        \n",
    "        self.fc2_out = 1000\n",
    "        \n",
    "        \n",
    "        # Trivial CNN just for testing that everything works\n",
    "        self.conv1 = nn.Conv2d(self.input_ch,self.conv1_out,kernel_size=3,stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(self.conv1_out)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "          \n",
    "        self.conv2 = nn.Conv2d(self.conv1_out,self.conv2_out,kernel_size=3,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.conv2_out)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(self.conv2_out,self.conv3_out,kernel_size=2,stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(self.conv3_out)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in, self.fc1_out)\n",
    "        self.bn4 = nn.BatchNorm1d(self.fc1_out)\n",
    "        self.do1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.fc1_out, self.fc2_out)\n",
    "        self.bn5 = nn.BatchNorm1d(self.fc2_out)\n",
    "        self.do2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # 14 classes\n",
    "        self.last = nn.Linear(self.fc2_out, 14)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(-1, self.fc1_in)\n",
    "        x = self.do1(torch.relu(self.bn4(self.fc1(x))))\n",
    "        x = self.do2(torch.relu(self.bn5(self.fc2(x))))\n",
    "        # Sigmoid to make all values between 0 and 1\n",
    "        x = torch.sigmoid(self.last(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".  [See more here on how to load the model back](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) if you want to continue training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You should return your predictions for the test set in a plain text file.  The text file contains one row for each test set image.  Each row contains a binary prediction for each label (separated by a single space), 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "The order of the rows should be according to the numeric order of the image numbers.  In the test set, this means that the first row refers to image `im20001.jpg`, the second to `im20002.jpg`, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the prediction output matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
